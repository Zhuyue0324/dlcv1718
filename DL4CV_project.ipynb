{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection in the street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is the main notebook (temporary just a structure now of all steps that we can do) for our DL4CV project. Our aim is to detect the object in the street by means of a convolutional neural network. The detection will be like a segmentation task with find out what each pixel of the object in a photo belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.3 |Anaconda custom (64-bit)| (default, Oct 13 2017, 12:02:49) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, sampler\n",
    "from MyFolder import MyImageFolder\n",
    "from visualisation import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This step should be to load the data images and the label images (by pixel), while doing feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--In leftImg8bit.zip, the train and test images are all 8-bit 2048*1024 pixels png type images. \n",
    "\n",
    "--In gtCoarse, the train and test labels are all 2048*1024 pixels png type images, which ONLY color the road in pink and the small objects on it in blue, the other pixels are black.\n",
    "\n",
    "-- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELSCSV = pd.read_csv(\"labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 10)\n"
     ]
    }
   ],
   "source": [
    "print(LABELSCSV.shape)\n",
    "#print(labelscsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pre-training of the images contains the following steps:\n",
    "\n",
    "--transforms each pixel from [0,255] to [0,1] \n",
    "\n",
    "--Normalisation for resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose(\n",
    "#    [transforms.Scale(256),\n",
    "#     [transforms.CenterCrop(224),     \n",
    "     [transforms.ToTensor()])\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "TARGET_TRANSFORM = transforms.Compose(\n",
    "#    [transforms.Scale(256),\n",
    "#     [transforms.CenterCrop(224),  \n",
    "     [transforms.ToTensor(),\n",
    "     transforms.Normalize((0, 0, 0), (1/255, 1/255, 1/255))])\n",
    "\n",
    "ROAD_LABELS = [1,2,7,8,9,10]#remove 8,9,10 to only keep the road\n",
    "# WARNING does not change lowest_non_road_color yet\n",
    "\n",
    "ROOT = '/mnt/disks/sdb1/cityscapes/'\n",
    "LI8B = 'leftImg8bit/'\n",
    "GT = 'gtFine/'\n",
    "DATA_TRAIN = MyImageFolder(root1=ROOT+LI8B+'train', root2=ROOT+GT+'train_lido' , transform = TRANSFORM, target_transform= TARGET_TRANSFORM)\n",
    "DATA_VAL = MyImageFolder(root1=ROOT+LI8B+'val', root2=ROOT+GT+'val_lido' , transform = TRANSFORM, target_transform= TARGET_TRANSFORM)\n",
    "DATA_TEST = MyImageFolder(root1=ROOT+LI8B+'test', root2=ROOT+GT+'test_lido' , transform = TRANSFORM, target_transform= TARGET_TRANSFORM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of the data after feature-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  146\n",
      "Validation size:  23\n",
      "test size:  42\n",
      "Img size:  torch.Size([3, 1024, 2048])\n",
      "Segmentation size:  torch.Size([1024, 2048])\n",
      "9946\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAACuCAYAAAB6IQo+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5518b1b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X,Y=DATA_TRAIN[0]\n",
    "print(\"Train size: \", len(DATA_TRAIN))\n",
    "print(\"Validation size: \", len(DATA_VAL))\n",
    "print(\"test size: \", len(DATA_TEST))\n",
    "print(\"Img size: \", X.size())\n",
    "print(\"Segmentation size: \", Y.size())\n",
    "\n",
    "plt.figure(figsize=(10, 15))\n",
    "\n",
    "\"\"\"\n",
    "# img\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.imshow(X.numpy().transpose(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.title(\"Input image\")\n",
    "\"\"\" \n",
    "\n",
    "# segmentation target\n",
    "TARGET = reduce(Y.numpy(), ROAD_LABELS)\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.imshow(TARGET)\n",
    "plt.axis('off')\n",
    "plt.title(\"Segmented image\")\n",
    "\n",
    "# test compare\n",
    "TRANSLATED = np.copy(TARGET)\n",
    "TRANSLATED[100:1000,:] = TRANSLATED[0:900,:]\n",
    "B = compare(TARGET,TRANSLATED)\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.imshow(B)\n",
    "plt.title(\"Test\")\n",
    "IM = Image.fromarray(B)\n",
    "IM.save(\"compared lines.jpeg\")\n",
    "\"\"\"\n",
    "# test lnr_basic\n",
    "A = lnr_basic(TARGET)\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.axis('off')\n",
    "plt.imshow(A)\n",
    "plt.title(\"Test\")\n",
    "IM = Image.fromarray(A)\n",
    "IM.save(\"your_file.jpeg\")\n",
    "\n",
    "# stixel target\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.axis('off')\n",
    "#plt.imshow(X.numpy().transpose(1,2,0))\n",
    "plt.imshow(lowest_non_road_color(X.numpy(), Y.numpy()))\n",
    "plt.title(\"Road limits\")\n",
    "\"\"\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we construct our net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Mynet34 import PretrainedResNet34, MyNet\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "NB_TRAIN=len(DATA_TRAIN)\n",
    "NB_VAL=len(DATA_VAL)\n",
    "NB_TEST=len(DATA_TEST)\n",
    "\n",
    "WEIGHTS_PATH = \"w34s32w128.pth\"\n",
    "# where the weights are saved in the end, for further reuse\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a pretrained Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRETRAINED_NET = PretrainedResNet34()\n",
    "PRETRAINED_NET.load_state_dict(models.resnet34(pretrained=True).state_dict())\n",
    "NET = MyNet(NUM_CLASSES, PRETRAINED_NET)\n",
    "if USE_GPU:\n",
    "    NET.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CRITERION = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "if USE_GPU:\n",
    "    CRITERION.cuda()\n",
    "OPTIMIZER = optim.SGD(NET.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "TRAIN_LOADER = DataLoader(DATA_TRAIN, batch_size=BATCH_SIZE, sampler=sampler.RandomSampler(DATA_TRAIN))\n",
    "VAL_LOADER = DataLoader(DATA_VAL, batch_size=BATCH_SIZE)\n",
    "TEST_LOADER = DataLoader(DATA_TEST, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a pretrained model if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained weights found at:w34s32w128.pth\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "if os.path.exists(WEIGHTS_PATH):\n",
    "    NET.load_state_dict(torch.load(WEIGHTS_PATH))\n",
    "    print(\"Loaded weights at:\"+WEIGHTS_PATH)\n",
    "else:\n",
    "    print(\"No pretrained weights found at:\"+WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate the StixelNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Slice width:  2048\tStride:    32 \tNumber of Slices:    61\n",
      "[1, 10240] loss: 0.19424\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-967bca51518f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mepochloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnumsample\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "NEEDS_NEW_EPOCH= True\n",
    "LAST_TRAIN_LOSS=10\n",
    "LAST_VAL_LOSS=10\n",
    "#EPOCH=0\n",
    "WIDTH=2048\n",
    "SLICE_WIDTH=128\n",
    "STRIDE=32\n",
    "NUM_SLICES = (WIDTH - SLICE_WIDTH)//STRIDE +1    #added   \"Integer Division\"\n",
    "PRINT_FREQUENCY=1024  \n",
    "for epoch in range(1):\n",
    "#while needNewEpoch:       \n",
    "    #--------------------------------------training period---------------------------------------\n",
    "    running_loss = 0.0\n",
    "    epochloss = 0.0\n",
    "    numsample=0\n",
    "     \n",
    "    NET.train()\n",
    "    \n",
    "    print (\"Slice width: %5d\\tStride: %5d \\tNumber of Slices: %5d\" %(WIDTH, STRIDE, NUM_SLICES))   #added  \n",
    "    \n",
    "    for inputs, labels in TRAIN_LOADER: \n",
    "        labels_masked = reduce(labels.numpy(), ROAD_LABELS)\n",
    "        labels = torch.from_numpy(labels_masked)\n",
    "    \n",
    "        for i in range (NUM_SLICES):   ### added         \n",
    "        \n",
    "            inputs_temp = inputs[:, :, :, i*STRIDE : i*STRIDE + SLICE_WIDTH]   ### added\n",
    "            labels_temp = labels[:, :, i*STRIDE : i*STRIDE + SLICE_WIDTH]   ### added      \n",
    "            \n",
    "            if USE_GPU:   \n",
    "                inputs_temp = inputs_temp.cuda()    ### changed\n",
    "                labels_temp = labels_temp.cuda()    ### changed\n",
    "            \n",
    "            inputs_temp, labels_temp = Variable(inputs_temp), Variable(labels_temp)     ### changed       \n",
    "            # zero the parameter gradients\n",
    "            OPTIMIZER.zero_grad()      \n",
    "            # forward + backward + optimize\n",
    "            outputs = NET(inputs_temp)                  ### changed\n",
    "            loss = CRITERION(outputs, labels_temp)      ### changed\n",
    "            loss.backward()\n",
    "            OPTIMIZER.step()            \n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            epochloss+=loss.data[0]\n",
    "            numsample += BATCH_SIZE\n",
    "            if numsample % PRINT_FREQUENCY == 0: #printfrequence-1: \n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch+1, numsample, running_loss / PRINT_FREQUENCY),\n",
    "                     end='\\r', flush=True)\n",
    "                running_loss = 0.0\n",
    "                torch.save(NET.state_dict(),WEIGHTS_PATH[:-4]+\"_temp.pth\")\n",
    "\n",
    "    newTrainLoss = epochloss/(NB_TRAIN*NUM_SLICES)   #changed\n",
    "    print('The average loss of epoch ', epoch+1, ' is ', newTrainLoss)\n",
    "    torch.save(NET.state_dict(),WEIGHTS_PATH)\n",
    "    #--------------------------------------validation period---------------------------------------\n",
    "    meanCorrectProba = 0.0\n",
    "    epochloss = 0.0\n",
    "    numsample=0\n",
    "    NET.eval()\n",
    "    print (\"Slice width:  %5d Number of Slices:  %5d\" %(WIDTH, NUM_SLICES))   #added \n",
    "    \n",
    "    for inputs, labels in VAL_LOADER:\n",
    "        labels_masked = reduce(labels.numpy(), ROAD_LABELS)\n",
    "        labels = torch.from_numpy(labels_masked)\n",
    "\n",
    "        for i in range (NUM_SLICES): #added\n",
    "\n",
    "            inputs_temp = inputs[:, :, :, i*STRIDE : i*STRIDE + SLICE_WIDTH]   ### added\n",
    "            labels_temp = labels[:, :, i*STRIDE : i*STRIDE + SLICE_WIDTH]   ### added    \n",
    "            \n",
    "            if USE_GPU:\n",
    "                inputs_temp = inputs_temp.cuda()    #changed\n",
    "                labels_temp = labels_temp.cuda()    #changed    \n",
    "                \n",
    "            inputs_temp, labels_temp = Variable(inputs_temp), Variable(labels_temp) #changed\n",
    "            outputs=NET(inputs_temp)    #changed\n",
    "            loss = CRITERION(outputs, labels_temp) #changed\n",
    "            meanProbability=np.exp(-loss.data[0])\n",
    "            epochloss += loss.data[0]\n",
    "            meanCorrectProba += meanProbability\n",
    "            numsample += BATCH_SIZE\n",
    "       \n",
    "    newValLoss = epochloss / (NB_VAL*NUM_SLICES) \n",
    "    print('The average validation loss is ', newValLoss)\n",
    "    print('The average correctness of the validation data is ', meanCorrectProba/(NB_VAL*NUM_SLICES)*100, '%')   #changed\n",
    "    #--------------------------------------evaluate the necessity of a new epoch---------------------------------------\n",
    "    if (LAST_VAL_LOSS-newValLoss<0.01) and (LAST_TRAIN_LOSS-newTrainLoss<0.01):\n",
    "        needNewEpoch=False\n",
    "    else:\n",
    "        lastloss=newValLoss \n",
    "        #epoch=epoch+1\n",
    "    LAST_VAL_LOSS = newValLoss\n",
    "    LAST_TRAIN_LOSS = newTrainLoss\n",
    "\n",
    "print(\"End training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 8, not 241",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e52972d4ced4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNB_SAMPLE_IMGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/sdb1/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1054\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/sdb1/anaconda3/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/sdb1/anaconda3/lib/python3.6/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m                     raise ValueError(\n\u001b[1;32m     63\u001b[0m                         \"num must be 1 <= num <= {maxn}, not {num}\".format(\n\u001b[0;32m---> 64\u001b[0;31m                             maxn=rows*cols, num=num))\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subplotspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m# num - 1 for converting from MATLAB to python indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 8, not 241"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa643de8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NB_SAMPLE_IMGS = 2\n",
    "plt.figure(figsize=(15, 4 * NB_SAMPLE_IMGS))\n",
    "OFFSET = SLICE_WIDTH - 2 * STRIDE\n",
    "for i in range (NB_SAMPLE_IMGS):\n",
    "    x,y=DATA_VAL[i]\n",
    "    inputs = x.unsqueeze(0)\n",
    "    outputs = inputs[:,0,:,:].clone()\n",
    "    \n",
    "    labels_masked = reduce(labels.numpy(), ROAD_LABELS)\n",
    "    labels = torch.from_numpy(labels_masked)\n",
    "    for i in range (NUM_SLICES): #added\n",
    "\n",
    "            inputs_temp = inputs[:, :, :, i*STRIDE : i*STRIDE + SLICE_WIDTH]   ### added\n",
    "            labels_temp = labels[:, :, i*STRIDE : i*STRIDE + SLICE_WIDTH]   ### added    \n",
    "            \n",
    "            if USE_GPU:\n",
    "                inputs_temp = inputs_temp.cuda()    #changed\n",
    "                labels_temp = labels_temp.cuda()    #changed    \n",
    "                \n",
    "            inputs_temp, labels_temp = Variable(inputs_temp), Variable(labels_temp) #changed\n",
    "            #print(outputs[:, :, :, i*STRIDE+OFFSET : (i+1)*STRIDE+OFFSET].shape)\n",
    "            #print(NET(inputs_temp).data[:, :, :, OFFSET : STRIDE+OFFSET].shape)\n",
    "            outputs[:, :, i*STRIDE+OFFSET : (i+1)*STRIDE+OFFSET]=\\\n",
    "            NET(inputs_temp).data[:, 0, :, OFFSET : STRIDE+OFFSET]\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    pred = preds[0].cpu()\n",
    "    print(pred.shape)\n",
    "    img, target = x.numpy(), y.numpy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # img\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 4, i * 4 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.transpose(1,2,0))\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "   \n",
    "\n",
    "    # target\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 4, i * 4 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(lowest_non_road_color(img, target))\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "    \"\"\" \"\"\"\n",
    "    # target reduced to 2 classes\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 4, i * 4 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(reduce(target, ROAD_LABELS))\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "        \n",
    "    # pred\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 4, i * 4 + 4)\n",
    "    plt.axis('off')\n",
    "    plt.imshow((pred.numpy()))\n",
    "    if i == 0:\n",
    "        plt.title(\"Prediction image\")\n",
    "        \n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ARCHIVE] Sandbox\n",
    "\n",
    "_The following is not supposed to be run, only kept for reference_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate the SegmentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "needNewEpoch= True\n",
    "lastTrainLoss=10\n",
    "lastValLoss=10\n",
    "epoch=0\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "#while needNewEpoch:       \n",
    "    #--------------------------------------training period---------------------------------------\n",
    "    running_loss = 0.0\n",
    "    epochloss = 0.0\n",
    "    numsample=0\n",
    "    printfrequence=20\n",
    "    \n",
    "    net.train()\n",
    "    for inputs, labels in trainloader:\n",
    "        if useGPU:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "   \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        epochloss+=loss.data[0]\n",
    "        numsample += batchsize\n",
    "        if numsample % printfrequence == 0: #printfrequence-1: \n",
    "            print('[%d, %5d] loss: %.5f' % (epoch+1, numsample, running_loss / printfrequence))\n",
    "            running_loss = 0.0\n",
    "    newTrainLoss = epochloss/nbtrain        \n",
    "    print('The average loss of epoch ', epoch+1, ' is ', newTrainLoss)\n",
    "    torch.save(net.state_dict(),weightpath)\n",
    "    #--------------------------------------validation period---------------------------------------\n",
    "    meanCorrectProba = 0.0\n",
    "    epochloss = 0.0\n",
    "    numsample=0\n",
    "    printfrequence=200\n",
    "    \n",
    "    net.eval()\n",
    "    for inputs, labels in valloader:\n",
    "        if useGPU:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs=net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        meanProbability=np.exp(-loss.data[0])\n",
    "        \n",
    "        # print statistics\n",
    "        epochloss += loss.data[0]\n",
    "        meanCorrectProba += meanProbability\n",
    "        numsample += batchsize\n",
    "        #if numsample % printfrequence == 0: #printfrequence-1: \n",
    "        #    print(numsample, ' validation images passed')\n",
    "    newValLoss = epochloss / nbval\n",
    "    print('The average validation loss is ', newValLoss)\n",
    "    print('The average correctness of the validation data is ', meanCorrectProba/nbval*100, '%')\n",
    "    #--------------------------------------evaluate the necessity of a new epoch---------------------------------------\n",
    "    if (lastValLoss-newValLoss<0.01) and (lastTrainLoss-newTrainLoss<0.01):\n",
    "        needNewEpoch=False\n",
    "    else:\n",
    "        lastLoss=newValLoss \n",
    "        #epoch=epoch+1\n",
    "\n",
    "print(\"End training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meanCorrectProba = 0.0\n",
    "numsample=0\n",
    "printfrequence=20\n",
    "    \n",
    "net.eval()\n",
    "for inputs, labels in trainloader:\n",
    "    if useGPU:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    inputs, labels = Variable(inputs), Variable(labels)\n",
    "    outputs=net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    meanProbability=np.exp(-loss.data[0])\n",
    "        \n",
    "    # print statistics\n",
    "    meanCorrectProba += meanProbability\n",
    "    numsample += batchsize\n",
    "#    if numsample % printfrequence == 0: #printfrequence-1: \n",
    "#        print('[%d, %5d] loss: %.5f' % (epoch+1, numsample, running_loss / printfrequence))\n",
    "#        running_loss = 0.0\n",
    "print('The average correctness of the validation data is ', meanCorrectProba/nbval*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_SAMPLE_IMGS = 4\n",
    "plt.figure(figsize=(15, 5 * NB_SAMPLE_IMGS))\n",
    "for i in range (NB_SAMPLE_IMGS):\n",
    "    x,y=dataval[i]\n",
    "    inputs = x.unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Segmentation\n",
    "    inputs = Variable(inputs)\n",
    "    if useGPU:\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    pred = preds[0].data.cpu()\n",
    "    \"\"\"\n",
    "    # Stixels\n",
    "    outputs = inputs.copy()\n",
    "    for i in range(num_slices):\n",
    "        inputs_temps = Variable(inputs[:,:,:,i*slice_width : (i+1)*slice_width0])\n",
    "        if useGPU:\n",
    "            inputs_temp = inputs_temp.cuda()\n",
    "        outputs[:,:,:,i*slice_width : (i+1)*slice_width0] = \\\n",
    "        net(inputs_temp)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    pred = preds[0].data.cpu()\n",
    "    \n",
    "    \n",
    "    img, target, pred = x.numpy(), y.numpy(), pred.numpy()\n",
    "    \n",
    "    # img\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 3, i * 3 + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.transpose(1,2,0))\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 3, i * 3 + 2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(label_img_to_rgb(target))\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "\n",
    "    # pred\n",
    "    plt.subplot(NB_SAMPLE_IMGS, 3, i * 3 + 3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(label_img_to_rgb(pred))\n",
    "    if i == 0:\n",
    "        plt.title(\"Prediction image\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. find the lowest non road pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "newseuil=torch.Tensor(numlabels)\n",
    "\n",
    "ConfusionMatrixTrain=[torch.Tensor() for i in range(numlabels)]\n",
    "ConfusionMatrixValidation=[torch.Tensor() for i in range(numlabels)]\n",
    "ConfusionMatrixTest=[torch.Tensor() for i in range(numlabels)]\n",
    "APM=Meter.APMeter()\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "       \n",
    "    #--------------------------------------training period---------------------------------------\n",
    "    running_loss = 0.0\n",
    "    epochloss = 0.0\n",
    "    numsample=0\n",
    "    printfrequence=10000\n",
    "    for i in range(numlabels):\n",
    "        ConfusionMatrixTrain[i]=torch.IntTensor(2,2).zero_()\n",
    "    \n",
    "    net.train()\n",
    "    for inputs, labels in trainloader:\n",
    "        if useGPU:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "   \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #trainging confusion matrix\n",
    "        result=FinalResult(outputs.data.cpu(), seuil.repeat(outputs.data.cpu().size(0),1))\n",
    "        for j in range(labels.data.cpu().size(0)):\n",
    "            for i in range(numlabels):\n",
    "                ConfusionMatrixTrain[i][int(labels.data.cpu()[j][i])][int(result.cpu()[j][i])]+=1\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        epochloss+=loss.data[0]\n",
    "        numsample += batchsize\n",
    "        if numsample % printfrequence == 0: #printfrequence-1:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' % (epoch+1, numsample, running_loss / printfrequence))\n",
    "            running_loss = 0.0\n",
    "    print('The average loss of epoch ', epoch+1, ' is ', epochloss/nbtrain)\n",
    "    print('The confusion matrixs for training are: ', ConfusionMatrixTrain)\n",
    "    traingraph.append(epochloss/nbtrain)\n",
    "    ConMatTrain.append(ConfusionMatrixTrain.copy())\n",
    "    torch.save(net.state_dict(),weightpath)\n",
    "    \n",
    "    #--------------------------------------Validation period---------------------------------------\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    epochloss = 0.0\n",
    "    numsample=0\n",
    "    printfrequence=10000\n",
    "    for i in range(numlabels):\n",
    "        newseuil[i]=0.0\n",
    "        ConfusionMatrixValidation[i]=torch.IntTensor(2,2).zero_()\n",
    "    \n",
    "    net.eval()\n",
    "    for inputs, labels in validationloader:\n",
    "        if useGPU:\n",
    "            inputs = inputs.cuda()\n",
    "        outputs=net(Variable(inputs))\n",
    "        result=FinalResult(outputs.data.cpu(), seuil.repeat(outputs.data.cpu().size(0),1))\n",
    "        \n",
    "        #validation confusion matrix\n",
    "        for j in range(labels.cpu().size(0)):\n",
    "            newseuil+=outputs.data.cpu()[j]\n",
    "            numsample+=1\n",
    "            for i in range(numlabels):\n",
    "                ConfusionMatrixValidation[i][int(labels.cpu()[j][i])][int(result.cpu()[j][i])]+=1\n",
    "\n",
    "  \n",
    "    newseuil=newseuil/numsample\n",
    "    seuil=newseuil.clone()\n",
    "    print('The seuil of epoch', epoch+1, ' is defined at', seuil)\n",
    "    ConMatValidation.append(ConfusionMatrixValidation.copy())\n",
    "    #print('The confusion matrixs for validation are: ', ConfusionMatrixvalidation)\n",
    "    \n",
    "    #--------------------------------------Test period---------------------------------------\n",
    "    \n",
    "    APM.reset()\n",
    "    numsample=0\n",
    "    printfrequence=10000\n",
    "    classerror=torch.zeros(numlabels)\n",
    "    for i in range(numlabels):\n",
    "        ConfusionMatrixTest[i]=torch.IntTensor(2,2).zero_()\n",
    "        \n",
    "    net.eval()\n",
    "    for inputs, labels in testloader:\n",
    "        if useGPU:\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        outputs=net(Variable(inputs))\n",
    "        result=FinalResult(outputs.data.cpu(), seuil.repeat(outputs.data.cpu().size(0),1))\n",
    "        \n",
    "        for j in range(labels.size(0)):\n",
    "            classerror.add_(torch.Tensor.float((labels[j]-result.cpu()[j]).abs()))\n",
    "            numsample +=1\n",
    "            APM.add(outputs.data.cpu()[j], labels[j])\n",
    "            for i in range(numlabels):\n",
    "                ConfusionMatrixTest[i][int(labels[j][i])][int(result.cpu()[j][i])]+=1\n",
    "        \n",
    "        if numsample % printfrequence == 0: #printfrequence-1:    # print every 100 mini-batches\n",
    "            print(numsample, 'images passed')\n",
    "\n",
    "    \n",
    "    print('In total we have ', numsample, 'images for the test.')\n",
    "    print(\"The Average Precision are \", APM.value())\n",
    "    #print(\"The correctness per class are \", torch.ones(numlabels).sub(classerror.div_(numsample)))\n",
    "    print(\"The mean test loss is \", classerror.div_(numsample).mean())\n",
    "    print('The confusion matrixs for test are: ', ConfusionMatrixTest)\n",
    "    ConMatTest.append(ConfusionMatrixTest.copy())\n",
    "    testgraph.append(classerror.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot([1,2,3,4])\n",
    "plt.axis([0, 10, 0, 0.5])\n",
    "\n",
    "plt.plot(traingraph, label=\"training loss\")\n",
    "plt.plot(testgraph, label=\"test loss\")\n",
    "legend = plt.legend()\n",
    "\n",
    "plt.ylabel('mean loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.plot([1,2,3,4])\n",
    "plt.axis([0, 10, 0.75, 1])\n",
    "\n",
    "plt.plot(x1, label=\"train lisse-dente\")\n",
    "plt.plot(x2, label=\"train alterne-oppose\")\n",
    "plt.plot(x3, label=\"train simple-composee\")\n",
    "plt.plot(x4, label=\"train non ligneux-ligneux\")\n",
    "\n",
    "plt.plot(y1, label=\"test lisse-dente\")\n",
    "plt.plot(y2, label=\"test alterne-oppose\")\n",
    "plt.plot(y3, label=\"test simple-composee\")\n",
    "plt.plot(y4, label=\"test non ligneux-ligneux\")\n",
    "\n",
    "legend = plt.legend()\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Correctness par class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
